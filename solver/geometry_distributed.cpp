/* DGPR: Discontinuous Galerkin Performance Research                          */
/* Copyright (C) 2023  Miles McGruder                                         */
/*                                                                            */
/* This program is free software: you can redistribute it and/or modify       */
/* it under the terms of the GNU General Public License as published by       */
/* the Free Software Foundation, either version 3 of the License, or          */
/* (at your option) any later version.                                        */
/*                                                                            */
/* This program is distributed in the hope that it will be useful,            */
/* but WITHOUT ANY WARRANTY; without even the implied warranty of             */
/* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the              */
/* GNU General Public License for more details.                               */
/*                                                                            */
/* You should have received a copy of the GNU General Public License          */
/* along with this program.  If not, see <https://www.gnu.org/licenses/>.     */

#pragma once

#include <mpi.h>

#include "basis.cpp"
#include "boundaries.cpp"
#include "dgmath.cpp"
#include "error.cpp"
#include "geometry_common.cpp"

/* set floating point MPI type (replacing MPI_REAL b/c it's a Fortran type) */

#undef MPI_REAL

#if SINGLE_PRECISION
#define MPI_REAL MPI_FLOAT
#else
#define MPI_REAL MPI_DOUBLE
#endif

// This struct stores almost all information necessary for simulation execution
// (everything short of fluid properties and reference conditions I believe).
// All of this information is geometry (including, say, boundary types), hence
// the name. Pre-computed reference space information is stored in basis structs
// along with all necessary pre-computed global space information. Some areas of
// the code may choose to ignore pre-computing and just store basis and geometry
// node information, this is useful for plotting.
struct distributed_geometry
{
  core_geometry core;

  MPI_Comm comm;

  // global info
  u64 nx_glo;
  u64 ny_glo;
  u64 nz_glo;
  s64 bounds_glo[6];

  u64 proc;  // processor number
  u64 px;    // processor iterator in x
  u64 py;    //                       y
  u64 pz;    //                       z

  // MPI data types for the distribution of state and gradient data between
  // domains in each direction. These data types are set up on initialization
  // and perform the indexing for all face transfers automatically. One datatype
  // is provided for each direction in -x +x -y +y -z +z order (same order as
  // other boundary storage data). Considering an array with a single entry for
  // each quadrature node on each face in the following order:
  //  each face (same order as face list)
  //    each quadrature point (same order as "quad" struct)
  // these types perform indexing automatically during transfers of boundary
  // quantities between arrays of this form.
  // NOTE: in order for these data types to index correctly into the face lists
  // on different processors it is assumed that the order of appearance in the
  // face list of corresponding faces on different processors is the same. This
  // is ensured currently because the faces are generated by iterating over
  // elements in [z [y [x]]] order.
  MPI_Datatype tfer_dtypes[6];

  // The face storage order (TODO: document this) is intended to keep
  // memory access local in the overall state list but it's a bit...
  // complicated, and I'm not sure of an easy (or quick) way to resolve a face
  // index into neighboring elements. So this is the list that says, what the
  // neighboring element (or boundary type) is for any global face along with
  // the local face numbers on either side of the face.
  //   for every element face (in the above mentioned order)
  //     [element_left element_right local_face_left local_face_right]
  // There is guarunteed to be an element on the left but the right element may
  // be a boundary, in this case the element_right entry will be the boundary
  // type (stored as a negative number) and the local_face_right entry is
  // undefined. Boundary types are defined in the btype enum.

  // The following face lists store the same information as the overall face
  // list above except the faces are split into interior and boundary lists.
  // Interior in this case means that the face is inside the domain even if it
  // may be on the edge of a particular processor.

  // [face_number elem_left elem_right local_face_left local_face_right]
  // Normally elem_right stores the element on the right, in the event that the
  // face represents a face on the edge of this domain and there is no known
  // element on the right, elem_right stores the index of the auxiliary mass
  // matrix for the element on the other side negated and with 1 subtracted. One
  // needs to be subtracted so there's no ambiguity at zero.
  array<s64> interior_face_list;

  // [face_number elem_left bound_type local_face_left]
  array<s64> boundary_face_list;

  // Normal vectors at each face quadrature point on each face in the geometry.
  // The normal vector will always point from the left element to the right
  // element (as listed in the face_list) (boundaries are considered to be on
  // the right). All normal vectors are (or should be) normalized during
  // pre-computation.
  // The format is as follows:
  //   each edge in the magic order (TODO: find a way to document this)
  //     each face quadrature point on theface
  array<real> n_;

  // Surface elements (surface integral Jacobian (is that technically correct?))
  // for face integration on at each face quadrature point on each face in the
  // goemetry. Storage is in the same order as the normals.
  array<real> fJ_;

  // Stores mass matrices of elements over interior face processor boundaries.
  // These mass matrices need to be stored because BR2 requires mass matrix
  // information to evaluate the stabilization term on both sides of a face and
  // each processor does its own geometry processing. You could solve this
  // problem by convoluted communication in the residual evaluation but it seems
  // more efficient to just send over the required mass matrices during
  // precomputation.
  // TODO: document the layout
  array<real> Minv_aux_;

  // initialization
  distributed_geometry();
  distributed_geometry(u64 sol_order, u64 geo_order, u64 nx_glo, u64 ny_glo,
                       u64 nz_glo, btype* bounds_glo, MPI_Comm comm, u64 np,
                       u64 npx, u64 npy, u64 npz);

  // This function is not a destructor because the geometry object can be used
  // without pre-computing and the MPI_Datatypes are only allocated when
  // pre-computing, so there will be mpi errors if you don't pre-compute and
  // this is in a destructor.
  void free_dtypes();

  // Converts a local element number to a global element number.
  u64 elem_num_to_global(u64 e);

  // Determines the processor a given global element is on.
  u64 global_elem_num_to_proc(u64 ge, u64 npx, u64 npy, u64 npz);

  // Pre-computes all relevant information for residual computation assuming nx,
  // ny, nz, p, q, and nodes, are already filled and all other memory is
  // allocated.
  void precompute();

  // individual pre-computation functions
  // (sometimes you won't need all the pre-computes at once)
  void precompute_face_geometry();
  void precompute_distribute_aux_mass_matrices();

  // convenient pre-compute accessors
  real* n(u64 face, u64 qnode);
  real& fJ(u64 face, u64 qnode);
  real* Minv_aux(u64 entry);

  // counts of things
  u64 nface() const;
  u64 num_interior_faces() const;
  u64 num_boundary_faces() const;
  u64 num_Minv_aux() const;

  // convenient pre-compute size functions
  u64 Minv_aux_size() const;
  u64 fJ_size() const;
  u64 n_size() const;
  u64 interior_face_list_size() const;
  u64 boundary_face_list_size() const;
};

// pre-compute surface normals and surface elements for integration over faces
void distributed_geometry::precompute_face_geometry()
{
  u64 nx  = core.nx;
  u64 ny  = core.ny;
  u64 nz  = core.nz;
  u64 gfn = 0, ifn = 0, bfn = 0;
  s64 faces_to_compute[6] = {1, 3, 5, -1, -1, -1};

  // super lazy data type indexing allocation
  int* dtype_blocklen[6];
  int* dtype_displace[6];
  u64 dtype_nface[6];
  u64 dtype_iface[6] = {};
  for (u64 i = 0; i < 6; ++i)
  {
    switch (i)
    {
      case 0:
      case 1: dtype_nface[i] = ny * nz; break;
      case 2:
      case 3: dtype_nface[i] = nx * nz; break;
      case 4:
      case 5: dtype_nface[i] = nx * ny; break;
    }
    dtype_blocklen[i] = new int[dtype_nface[i]];
    dtype_displace[i] = new int[dtype_nface[i]];
  }

  for (u64 k = 0; k < nz; ++k)
  {
    for (u64 j = 0; j < ny; ++j)
    {
      for (u64 i = 0; i < nx; ++i)
      {
        u64 e = nx * ny * k + nx * j + i;

        // generate list of local faces to pre-compute for this element
        for (u64 fn = 3; fn < 6; ++fn)
          faces_to_compute[fn] = -1;
        if (i == 0)
          faces_to_compute[3] = 0;
        if (j == 0)
          faces_to_compute[4] = 2;
        if (k == 0)
          faces_to_compute[5] = 4;

        // pre-compute the appropriate list of faces
        for (u64 if2comp = 0; if2comp < 6; ++if2comp)
        {
          if (faces_to_compute[if2comp] == -1)
            continue;

          u64 lfn = (u64)faces_to_compute[if2comp];  // local face number

          s64 iadj, jadj, kadj;
          s64 bound_indx =
          find_boundary_index(i, j, k, lfn, nx, ny, nz, &iadj, &jadj, &kadj);

          if (bound_indx == -1 || core.bounds[bound_indx] >= 0)
          {
            interior_face_list[5 * ifn + 0] = gfn;
            interior_face_list[5 * ifn + 1] = e;
            interior_face_list[5 * ifn + 3] = lfn;
            interior_face_list[5 * ifn + 4] = opposite_local_face[lfn];

            // interior at domain edge
            if (bound_indx > -1 && core.bounds[bound_indx] >= 0)
            {
              interior_face_list[5 * ifn + 2] = -1;

              // building custom MPI datatypes
              dtype_blocklen[bound_indx][dtype_iface[bound_indx]] =
              core.refp.fqrule.n;
              dtype_displace[bound_indx][dtype_iface[bound_indx]] =
              gfn * core.refp.fqrule.n;
              ++dtype_iface[bound_indx];
            }
            // interior within this processor's domain
            else
            {
              interior_face_list[5 * ifn + 2] =
              nx * ny * kadj + nx * jadj + iadj;
            }

            ++ifn;
          }
          else
          {
            boundary_face_list[4 * bfn + 0] = gfn;
            boundary_face_list[4 * bfn + 1] = e;
            boundary_face_list[4 * bfn + 2] = core.bounds[bound_indx];
            boundary_face_list[4 * bfn + 3] = lfn;

            ++bfn;
          }

          // evaluate normal and surface element at each quad point
          for (u64 qnn = 0; qnn < core.refp.fqrule.n; ++qnn)
          {
            real* normal = n(gfn, qnn);
            real& surfJ  = fJ(gfn, qnn);

            compute_surface_geometry(e, lfn, qnn, core, iadj, jadj, kadj,
                                     normal, &surfJ);
          }
          ++gfn;
        }
      }
    }
  }

  // commit and free the datatypes
  MPI_Datatype tmp_types[6];  // stores types without adjusted extent
  for (u64 i = 0; i < 6; ++i)
  {
    if (core.bounds[i] > -1)
    {
      // create type with correct offsets but incorrect lower bound and extent
      //   (incorrect in that it can't be used in transfers of count > 1)
      MPI_Type_indexed(dtype_nface[i], dtype_blocklen[i], dtype_displace[i],
                       MPI_REAL, &tmp_types[i]);
      MPI_Type_commit(&tmp_types[i]);

      // create properly "extented" type
      MPI_Type_create_resized(tmp_types[i], 0,
                              sizeof(real) * nface() * core.refp.fqrule.n,
                              &tfer_dtypes[i]);
      MPI_Type_commit(&tfer_dtypes[i]);

      MPI_Type_free(&tmp_types[i]);
    }
    delete[] dtype_blocklen[i];
    delete[] dtype_displace[i];
  }
}

// communicate auxiliary mass matrices over interior faces
// NOTE: this is abusing the fact that the local face of an element on the
// edge of the domain will always correspond with the boundary index of this
// sub-domain. This seems scary because I think this depends on how the
// geometry is initially processed but not totally sure...
void distributed_geometry::precompute_distribute_aux_mass_matrices()
{
  /* send mass matrices (non-blocking because you might send to self) */

  u64 bitr[6]           = {};
  u64 Minvitr           = 0;
  MPI_Request* requests = new MPI_Request[num_Minv_aux()];

  for (u64 ifi = 0; ifi < num_interior_faces(); ++ifi)
  {
    s64 eL  = interior_face_list[5 * ifi + 1];
    s64 eR  = interior_face_list[5 * ifi + 2];
    s64 lfl = interior_face_list[5 * ifi + 3];

    if (eR < 0)
    {
      MPI_Issend(core.Minv(eL), core.num_entry_Minv(), MPI_REAL,
                 core.bounds[lfl], lfl * nface() + bitr[lfl], comm,
                 &requests[Minvitr]);

      ++bitr[lfl];
      ++Minvitr;
    }
  }

  /* receive mass matrices */

  for (u64 i = 0; i < 6; ++i)
    bitr[i] = 0;

  for (u64 ifi = 0; ifi < num_interior_faces(); ++ifi)
  {
    s64 eL  = interior_face_list[5 * ifi + 1];
    s64 eR  = interior_face_list[5 * ifi + 2];
    s64 lfl = interior_face_list[5 * ifi + 3];
    s64 lfr = interior_face_list[5 * ifi + 4];

    if (eR < 0)
    {
      u64 entry = (core.ny * core.nz * (core.bounds[0] > -1 && 0 < lfl)) +
                  (core.ny * core.nz * (core.bounds[1] > -1 && 1 < lfl)) +
                  (core.nx * core.nz * (core.bounds[2] > -1 && 2 < lfl)) +
                  (core.nx * core.nz * (core.bounds[3] > -1 && 3 < lfl)) +
                  (core.nx * core.ny * (core.bounds[4] > -1 && 4 < lfl)) +
                  (core.nx * core.ny * (core.bounds[5] > -1 && 5 < lfl)) +
                  bitr[lfl];

      interior_face_list[5 * ifi + 2] = -entry - 1;

      MPI_Recv(Minv_aux(entry), core.num_entry_Minv(), MPI_REAL,
               core.bounds[lfl], lfr * nface() + bitr[lfl], comm,
               MPI_STATUS_IGNORE);

      ++bitr[lfl];
    }
  }

  /* ensure messages received */

  for (u64 mi = 0; mi < num_Minv_aux(); ++mi)
    MPI_Wait(&requests[mi], MPI_STATUS_IGNORE);

  delete[] requests;
}

void distributed_geometry::precompute()
{
  core.precompute();
  precompute_face_geometry();
  precompute_distribute_aux_mass_matrices();
}

u64 distributed_geometry::elem_num_to_global(u64 e)
{
  u64 lex, ley, lez;
  split3_cartesian(e, core.nx, core.ny, &lex, &ley, &lez);

  u64 gex = core.nx * px + lex;
  u64 gey = core.ny * py + ley;
  u64 gez = core.nz * pz + lez;

  return nx_glo * ny_glo * gez + nx_glo * gey + gex;
}

u64 distributed_geometry::global_elem_num_to_proc(u64 ge, u64 npx, u64 npy,
                                                  u64 npz)
{
  u64 gex, gey, gez;
  split3_cartesian(ge, nx_glo, ny_glo, &gex, &gey, &gez);

  u64 pnx = gex / core.nx;  // truncation intentional
  u64 pny = gey / core.ny;  // *
  u64 pnz = gez / core.nz;  // *

  return npx * npy * pnz + npx * pny + pnx;
}
// pre-compute accessor functions ----------------------------------------------

real* distributed_geometry::n(u64 face, u64 qnode)
{
  return n_ + 3 * (core.refp.fqrule.n * face + qnode);
}
real& distributed_geometry::fJ(u64 face, u64 qnode)
{
  return fJ_[core.refp.fqrule.n * face + qnode];
}
real* distributed_geometry::Minv_aux(u64 entry)
{
  return Minv_aux_ + entry * core.num_entry_Minv();
}

// pre-compute array size functions --------------------------------------------

u64 distributed_geometry::nface() const
{
  return core.nelem * 3 + core.nx * core.ny + core.ny * core.nz +
         core.nz * core.nx;
}
u64 distributed_geometry::num_interior_faces() const
{
  return nface() - num_boundary_faces();
}
u64 distributed_geometry::num_boundary_faces() const
{
  return (core.ny * core.nz * (core.bounds[0] < -1)) +
         (core.ny * core.nz * (core.bounds[1] < -1)) +
         (core.nx * core.nz * (core.bounds[2] < -1)) +
         (core.nx * core.nz * (core.bounds[3] < -1)) +
         (core.nx * core.ny * (core.bounds[4] < -1)) +
         (core.nx * core.ny * (core.bounds[5] < -1));
}
u64 distributed_geometry::num_Minv_aux() const
{
  return (core.ny * core.nz * (core.bounds[0] > -1)) +
         (core.ny * core.nz * (core.bounds[1] > -1)) +
         (core.nx * core.nz * (core.bounds[2] > -1)) +
         (core.nx * core.nz * (core.bounds[3] > -1)) +
         (core.nx * core.ny * (core.bounds[4] > -1)) +
         (core.nx * core.ny * (core.bounds[5] > -1));
}

u64 distributed_geometry::Minv_aux_size() const
{
  return num_Minv_aux() * core.num_entry_Minv();
}
u64 distributed_geometry::fJ_size() const
{
  return nface() * core.refp.fqrule.n;
}
u64 distributed_geometry::n_size() const
{
  return nface() * core.refp.fqrule.n * 3;
}
u64 distributed_geometry::interior_face_list_size() const
{
  return num_interior_faces() * 5;
}
u64 distributed_geometry::boundary_face_list_size() const
{
  return num_boundary_faces() * 4;
}

// struct memory management implementaiton -------------------------------------

distributed_geometry::distributed_geometry() :
core(),
comm(MPI_COMM_NULL),
nx_glo(0),
ny_glo(0),
nz_glo(0),
bounds_glo{},
proc(0),
px(0),
py(0),
pz(0),
tfer_dtypes{},
interior_face_list(),
boundary_face_list(),
n_(),
fJ_(),
Minv_aux_()
{}

distributed_geometry::distributed_geometry(u64 sol_order, u64 geo_order,
                                           u64 nx_global, u64 ny_global,
                                           u64 nz_global, btype* bounds_global,
                                           MPI_Comm given_comm, u64 np, u64 npx,
                                           u64 npy, u64 npz)
{

  if (npx * npy * npz != np)
    errout("(npx : %d, npy : %d, npz : %d) does not match total "
           "processor count of %d!",
           npx, npy, npz, np);
  if (nx_global % npx != 0)
    errout("%d procs in x does not divide %d elems evenly!", npx, nx_global);
  if (ny_global % npy != 0)
    errout("%d procs in y does not divide %d elems evenly!", npy, ny_global);
  if (nz_global % npz != 0)
    errout("%d procs in z does not divide %d elems evenly!", npz, nz_global);

  if ((bounds_global[0] == periodic && bounds_global[1] != periodic) ||
      (bounds_global[1] == periodic && bounds_global[0] != periodic) ||
      (bounds_global[2] == periodic && bounds_global[3] != periodic) ||
      (bounds_global[3] == periodic && bounds_global[2] != periodic) ||
      (bounds_global[4] == periodic && bounds_global[5] != periodic) ||
      (bounds_global[5] == periodic && bounds_global[4] != periodic))
  {
    errout("periodic boundaries are not matching!");
  }

  comm = given_comm;

  int rank;
  MPI_Comm_rank(comm, &rank);
  proc = rank;

  nx_glo = nx_global;
  ny_glo = ny_global;
  nz_glo = nz_global;

  for (u64 i = 0; i < 6; ++i)
    bounds_glo[i] = bounds_global[i];

  // processor location iterators
  split3_cartesian(proc, npx, npy, &px, &py, &pz);

  // normal case, a neighboring processor is present
  s64 proc_domain_bounds[6];
  proc_domain_bounds[0] = (pz)*npx * npy + (py)*npx + ((px + npx - 1) % npx);
  proc_domain_bounds[1] = (pz)*npx * npy + (py)*npx + ((px + npx + 1) % npx);
  proc_domain_bounds[2] = (pz)*npx * npy + ((py + npy - 1) % npy) * npx + (px);
  proc_domain_bounds[3] = (pz)*npx * npy + ((py + npy + 1) % npy) * npx + (px);
  proc_domain_bounds[4] = ((pz + npz - 1) % npz) * npx * npy + (py)*npx + (px);
  proc_domain_bounds[5] = ((pz + npz + 1) % npz) * npx * npy + (py)*npx + (px);

  // special case, a processor domain bound is actually a global domain bound
  if (px == 0 && (bounds_glo[0] != periodic))
    proc_domain_bounds[0] = bounds_glo[0];
  if (px == npx - 1 && (bounds_glo[1] != periodic))
    proc_domain_bounds[1] = bounds_glo[1];
  if (py == 0 && (bounds_glo[2] != periodic))
    proc_domain_bounds[2] = bounds_glo[2];
  if (py == npy - 1 && (bounds_glo[3] != periodic))
    proc_domain_bounds[3] = bounds_glo[3];
  if (pz == 0 && (bounds_glo[4] != periodic))
    proc_domain_bounds[4] = bounds_glo[4];
  if (pz == npz - 1 && (bounds_glo[5] != periodic))
    proc_domain_bounds[5] = bounds_glo[5];

  core = core_geometry(sol_order, geo_order, nx_glo / npx, ny_glo / npy,
                       nz_glo / npz, proc_domain_bounds);

  Minv_aux_          = array<real>(Minv_aux_size());
  n_                 = array<real>(n_size());
  fJ_                = array<real>(fJ_size());
  interior_face_list = array<s64>(interior_face_list_size());
  boundary_face_list = array<s64>(boundary_face_list_size());
}

void distributed_geometry::free_dtypes()
{
  for (u64 i = 0; i < 6; ++i)
  {
    if (core.bounds[i] > -1)
      MPI_Type_free(&tfer_dtypes[i]);
  }
}

// goemetry communication ------------------------------------------------------

void scatter(core_geometry& geom_global, map<simstate>& outputs_global,
             MPI_Comm comm, u64 np, u64 npx, u64 npy, u64 npz,
             distributed_geometry& geom_local, map<simstate>& outputs_local)
{
  int rank;
  MPI_Comm_rank(comm, &rank);
  bool isroot = rank == 0;

  /* (bad) collective comms for metadata so new proc geoms can be constructed */

  int p      = (int)geom_global.p;
  int q      = (int)geom_global.q;
  int nx_glo = (int)geom_global.nx;
  int ny_glo = (int)geom_global.ny;
  int nz_glo = (int)geom_global.nz;
  int bounds_glo_int[6];
  for (u64 i = 0; i < 6; ++i)
    bounds_glo_int[i] = (int)geom_global.bounds[i];
  int noutput = 0;
  for (auto i = outputs_global.begin(); i != outputs_global.end(); ++i)
    ++noutput;

  MPI_Bcast(&p, 1, MPI_INT, 0, comm);
  MPI_Bcast(&q, 1, MPI_INT, 0, comm);
  MPI_Bcast(&nx_glo, 1, MPI_INT, 0, comm);
  MPI_Bcast(&ny_glo, 1, MPI_INT, 0, comm);
  MPI_Bcast(&nz_glo, 1, MPI_INT, 0, comm);
  MPI_Bcast(bounds_glo_int, 6, MPI_INT, 0, comm);
  MPI_Bcast(&noutput, 1, MPI_INT, 0, comm);

  btype bounds_glo[6];
  for (u64 i = 0; i < 6; ++i)
    bounds_glo[i] = (btype)bounds_glo_int[i];

  /* output initialization (with a little more comm for key names) */

  geom_local = distributed_geometry(p, q, nx_glo, ny_glo, nz_glo, bounds_glo,
                                    comm, np, npx, npy, npz);

  map<simstate>::iterator outputs_global_iterator;
  if (isroot)
    outputs_global_iterator = outputs_global.begin();
  for (int oi = 0; oi < noutput; ++oi)
  {
    char key[1024];
    int len;
    if (isroot)
    {
      outputs_global_iterator.key().copy_to(key);
      len = (int)(outputs_global_iterator.key().len()) + 1;
    }

    MPI_Bcast(key, 1024, MPI_CHAR, 0, comm);

    outputs_local.add(key, simstate(geom_local.core));

    if (isroot)
      ++outputs_global_iterator;
  }

  /* common constants */

  u64 geo_send_size = (q + 1) * (q + 1) * (q + 1) * 3;
  u64 sol_send_size = geom_local.core.refp.nbf3d * 5;
  u64 nelem_local =
  geom_local.core.nx * geom_local.core.ny * geom_local.core.nz;
  u64 nelem_global = nx_glo * ny_glo * nz_glo;

  MPI_Request* requests;
  if (isroot)
    requests = new MPI_Request[nelem_global];

  /* goemetry communication */

  if (isroot)
  {
    for (u64 gei = 0; gei < nelem_global; ++gei)
    {
      u64 proc = geom_local.global_elem_num_to_proc(gei, npx, npy, npz);
      MPI_Issend(geom_global.node(gei, 0), (int)geo_send_size, MPI_REAL,
                 (int)proc, gei, comm, &requests[gei]);
    }
  }

  for (u64 lei = 0; lei < nelem_local; ++lei)
  {
    u64 gei = geom_local.elem_num_to_global(lei);
    MPI_Recv(geom_local.core.node(lei, 0), (int)geo_send_size, MPI_REAL, 0, gei,
             comm, MPI_STATUS_IGNORE);
  }

  if (isroot)
  {
    for (u64 gei = 0; gei < nelem_global; ++gei)
      MPI_Wait(&requests[gei], MPI_STATUS_IGNORE);
  }

  MPI_Barrier(comm);

  /* output communication */

  for (auto i = outputs_local.begin(); i != outputs_local.end(); ++i)
  {
    if (isroot)
    {
      simstate& output_global = outputs_global.get(i.key());
      for (u64 gei = 0; gei < nelem_global; ++gei)
      {
        u64 proc = geom_local.global_elem_num_to_proc(gei, npx, npy, npz);
        MPI_Issend(&output_global(gei, 0, 0), (int)sol_send_size, MPI_REAL,
                   (int)proc, gei, comm, &requests[gei]);
      }
    }

    simstate& output_local = outputs_local.get(i.key());
    for (u64 lei = 0; lei < nelem_local; ++lei)
    {
      u64 gei = geom_local.elem_num_to_global(lei);
      MPI_Recv(&output_local(lei, 0, 0), (int)sol_send_size, MPI_REAL, 0, gei,
               comm, MPI_STATUS_IGNORE);
    }

    if (isroot)
    {
      for (u64 gei = 0; gei < nelem_global; ++gei)
        MPI_Wait(&requests[gei], MPI_STATUS_IGNORE);
    }

    MPI_Barrier(comm);
  }

  if (isroot)
    delete[] requests;
  MPI_Barrier(comm);
}

void gather(distributed_geometry& geom_local, map<simstate>& outputs_local,
            core_geometry& geom_global, map<simstate>& outputs_global)
{
  bool isroot       = geom_local.proc == 0;
  u64 qp1           = (geom_local.core.q + 1);
  u64 geo_send_size = qp1 * qp1 * qp1 * 3;
  u64 sol_send_size = geom_local.core.refp.nbf3d * 5;
  u64 nelem_local   = geom_local.core.nelem;
  u64 nelem_global  = geom_local.nx_glo * geom_local.ny_glo * geom_local.nz_glo;

  /* prepare global outputs */

  if (isroot)
  {
    geom_global =
    core_geometry(geom_local.core.p, geom_local.core.q, geom_local.nx_glo,
                  geom_local.ny_glo, geom_local.nz_glo, geom_local.bounds_glo);
    for (auto i = outputs_local.begin(); i != outputs_local.end(); ++i)
      outputs_global.add(i.key(), simstate(geom_global));
  }

  /* communicate geometry */

  MPI_Request* requests = new MPI_Request[nelem_local];

  for (u64 lei = 0; lei < nelem_local; ++lei)
  {
    u64 gei = geom_local.elem_num_to_global(lei);
    MPI_Issend(geom_local.core.node(lei, 0), (int)geo_send_size, MPI_REAL, 0,
               (int)gei, geom_local.comm, &requests[lei]);
  }

  if (isroot)
  {
    for (u64 gei = 0; gei < nelem_global; ++gei)
    {
      MPI_Recv(geom_global.node(gei, 0), (int)geo_send_size, MPI_REAL,
               MPI_ANY_SOURCE, (int)gei, geom_local.comm, MPI_STATUS_IGNORE);
    }
  }

  for (u64 lei = 0; lei < nelem_local; ++lei)
    MPI_Wait(&requests[lei], MPI_STATUS_IGNORE);

  MPI_Barrier(geom_local.comm);

  /* communicate each output */

  for (auto i = outputs_local.begin(); i != outputs_local.end(); ++i)
  {
    simstate& output_local = i.val();

    for (u64 lei = 0; lei < nelem_local; ++lei)
    {
      u64 gei = geom_local.elem_num_to_global(lei);
      MPI_Issend(&output_local(lei, 0, 0), (int)sol_send_size, MPI_REAL, 0,
                 (int)gei, geom_local.comm, &requests[lei]);
    }

    if (isroot)
    {
      simstate& output_global = outputs_global.get(i.key());

      for (u64 gei = 0; gei < nelem_global; ++gei)
      {
        MPI_Recv(&output_global(gei, 0, 0), (int)sol_send_size, MPI_REAL,
                 MPI_ANY_SOURCE, (int)gei, geom_local.comm, MPI_STATUS_IGNORE);
      }
    }

    for (u64 lei = 0; lei < nelem_local; ++lei)
      MPI_Wait(&requests[lei], MPI_STATUS_IGNORE);

    MPI_Barrier(geom_local.comm);
  }

  /* sync and exit */

  delete[] requests;
  MPI_Barrier(geom_local.comm);
}

// pre-defined domain definitions ----------------------------------------------

distributed_geometry bump_distributed(u64 p, u64 q, u64 nx, u64 ny, u64 nz,
                                      real lx, real ly, real lz, real a,
                                      real bmp_height, real bmp_center,
                                      real bmp_variance, btype* bounds,
                                      MPI_Comm comm, u64 np, u64 npx, u64 npy,
                                      u64 npz)
{
  if (p > MAX_ORDER)
  {
    errout(
    "requested solution order, %zu, is higher than the maximum order, %d!", q,
    MAX_ORDER);
  }
  if (q > MAX_ORDER)
  {
    errout(
    "requested geometry order, %zu, is higher than the maximum order, %d!", q,
    MAX_ORDER);
  }

  // initialize the stub of a geometry object, this allocs all memory but does
  // not set nodes or perform pre-computes
  distributed_geometry domain(p, q, nx, ny, nz, bounds, comm, np, npx, npy,
                              npz);

  // element sizes
  real elx = lx / (real)nx;
  real ely = ly / (real)ny;
  real elz = lz / (real)nz;

  // determine the lowest index corner element for this processor
  u64 dsx = domain.px * domain.core.nx;
  u64 dsy = domain.py * domain.core.ny;
  u64 dsz = domain.pz * domain.core.nz;

  generate_bump_nodes(elx, ely, elz, ly, domain.ny_glo, a, bmp_height, 
                      bmp_center, bmp_variance, dsx, dsy, dsz, domain.core);

  domain.precompute();

  return domain;
}
