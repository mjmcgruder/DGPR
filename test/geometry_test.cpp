/* DGPR: Discontinuous Galerkin Performance Research                          */
/* Copyright (C) 2023  Miles McGruder                                         */
/*                                                                            */
/* This program is free software: you can redistribute it and/or modify       */
/* it under the terms of the GNU General Public License as published by       */
/* the Free Software Foundation, either version 3 of the License, or          */
/* (at your option) any later version.                                        */
/*                                                                            */
/* This program is distributed in the hope that it will be useful,            */
/* but WITHOUT ANY WARRANTY; without even the implied warranty of             */
/* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the              */
/* GNU General Public License for more details.                               */
/*                                                                            */
/* You should have received a copy of the GNU General Public License          */
/* along with this program.  If not, see <https://www.gnu.org/licenses/>.     */

#pragma once

#include <cinttypes>

#include <mpitest.h>

#include "geometry_distributed.cpp"
#include "geometry_shared.cpp"

struct channel
{
  u64 p   = 2;
  u64 q   = 1;
  u64 nx  = 4;
  u64 ny  = 3;
  u64 nz  = 4;
  real lx = 2. * M_PI;
  real ly = 2.;
  real lz = 1. * M_PI;
  u64 npx = 1;
  u64 npy = 1;
  u64 npz = 1;

  btype bounds[6] = {periodic,      periodic, inviscid_wall,
                     inviscid_wall, periodic, periodic};

  distributed_geometry geom;

  channel()
  {}

  channel(MPI_Comm comm, u64 np)
  {
    int rank;
    MPI_Comm_rank(comm, &rank);

    if (np == 1)
    {
      npx = 1;
      npy = 1;
      npz = 1;
    }
    else if (np == 2)
    {
      npx = 2;
      npy = 1;
      npz = 1;
    }
    else if (np == 3)
    {
      npx = 1;
      npy = 3;
      npz = 1;
    }
    else if (np == 4)
    {
      npx = 2;
      npy = 1;
      npz = 2;
    }

    geom = bump_distributed(p, q, nx, ny, nz, lx, ly, lz, 0., 0., 0., 0.1,
                            bounds, comm, np, npx, npy, npz);
  }
};

TEST(geometry_test_neighboring_proc, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  switch (rank)
  {
    case 0:
      EXPECT_EQ(geom.core.bounds[0], (s64)1);
      EXPECT_EQ(geom.core.bounds[1], (s64)1);
      EXPECT_EQ(geom.core.bounds[2], (s64)inviscid_wall);
      EXPECT_EQ(geom.core.bounds[3], (s64)inviscid_wall);
      EXPECT_EQ(geom.core.bounds[4], (s64)2);
      EXPECT_EQ(geom.core.bounds[5], (s64)2);
      break;
    case 1:
      EXPECT_EQ(geom.core.bounds[0], (s64)0);
      EXPECT_EQ(geom.core.bounds[1], (s64)0);
      EXPECT_EQ(geom.core.bounds[2], (s64)inviscid_wall);
      EXPECT_EQ(geom.core.bounds[3], (s64)inviscid_wall);
      EXPECT_EQ(geom.core.bounds[4], (s64)3);
      EXPECT_EQ(geom.core.bounds[5], (s64)3);
      break;
    case 2:
      EXPECT_EQ(geom.core.bounds[0], (s64)3);
      EXPECT_EQ(geom.core.bounds[1], (s64)3);
      EXPECT_EQ(geom.core.bounds[2], (s64)inviscid_wall);
      EXPECT_EQ(geom.core.bounds[3], (s64)inviscid_wall);
      EXPECT_EQ(geom.core.bounds[4], (s64)0);
      EXPECT_EQ(geom.core.bounds[5], (s64)0);
      break;
    case 3:
      EXPECT_EQ(geom.core.bounds[0], (s64)2);
      EXPECT_EQ(geom.core.bounds[1], (s64)2);
      EXPECT_EQ(geom.core.bounds[2], (s64)inviscid_wall);
      EXPECT_EQ(geom.core.bounds[3], (s64)inviscid_wall);
      EXPECT_EQ(geom.core.bounds[4], (s64)1);
      EXPECT_EQ(geom.core.bounds[5], (s64)1);
      break;
  }
  data.geom.free_dtypes();
}

// This test works by generating a unique number for each quadrature point on
// each (sub-)domain boundary face (as a float), communicating those unique
// floats via the MPI datatypes generated by the geometry, and testing that the
// same code can be generated on the other end at exactly the expected index.
// This should ensure that the fill order of the datatypes correctly
// communicates face information on sub-domain bondaries between processors.
TEST(geometry_test_tfer_dtypes, 1, 2, 3, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  real* left  = new real[geom.nface() * geom.core.refp.fqrule.n * 5];
  real* right = new real[geom.nface() * geom.core.refp.fqrule.n * 5];

  /* populate initial arrays with unique encoding for each sent value */

  for (u64 ri = 0; ri < 5; ++ri)
  {
    u64 bfi[6] = {};
    for (u64 ifi = 0; ifi < geom.num_interior_faces(); ++ifi)
    {
      s64 fi              = geom.interior_face_list[5 * ifi + 0];
      s64 eR              = geom.interior_face_list[5 * ifi + 2];
      s64 lfl             = geom.interior_face_list[5 * ifi + 3];
      bool proc_over_face = eR < 0;
      for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
      {
        if (proc_over_face)
        {
          left[geom.core.refp.fqrule.n * geom.nface() * ri +
               geom.core.refp.fqrule.n * fi + qi] =
          (real)(6 * geom.nface() * geom.core.refp.fqrule.n * size * ri +
                 6 * geom.nface() * geom.core.refp.fqrule.n * rank +
                 geom.nface() * geom.core.refp.fqrule.n * lfl +
                 geom.core.refp.fqrule.n * bfi[lfl] + qi);
        }
        else
        {
          left[geom.core.refp.fqrule.n * geom.nface() * ri +
               geom.core.refp.fqrule.n * fi + qi] = (real)rank;
        }

        right[geom.core.refp.fqrule.n * geom.nface() * ri +
              geom.core.refp.fqrule.n * fi + qi] = -1.;
      }

      if (proc_over_face)
        ++bfi[lfl];
    }
  }

  /* communicate arrays */

  MPI_Request requests[6];
  for (u64 bi = 0; bi < 6; ++bi)
  {
    if (geom.core.bounds[bi] >= 0)
    {
      MPI_Issend(left, 5, geom.tfer_dtypes[bi], (int)geom.core.bounds[bi],
                 (int)bi, comm, &requests[bi]);
    }
  }

  for (u64 bi = 0; bi < 6; ++bi)
  {
    if (geom.core.bounds[bi] >= 0)
    {
      MPI_Status status;
      MPI_Recv(right, 5, geom.tfer_dtypes[bi], (int)geom.core.bounds[bi],
               (int)opposite_local_face[bi], comm, &status);
    }
  }

  for (u64 bi = 0; bi < 6; ++bi)
  {
    if (geom.core.bounds[bi] >= 0)
      MPI_Wait(&requests[bi], MPI_STATUS_IGNORE);
  }

  /* check received arrays */

  for (u64 ri = 0; ri < 5; ++ri)
  {
    u64 recv_bfi[6] = {};
    for (u64 ifi = 0; ifi < geom.num_interior_faces(); ++ifi)
    {
      s64 fi              = geom.interior_face_list[5 * ifi + 0];
      s64 eR              = geom.interior_face_list[5 * ifi + 2];
      s64 lfl             = geom.interior_face_list[5 * ifi + 3];
      bool proc_over_face = eR < 0;
      for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
      {
        if (proc_over_face)
        {
          EXPECT_EQ(
          right[geom.core.refp.fqrule.n * geom.nface() * ri +
                geom.core.refp.fqrule.n * fi + qi],
          (real)(
          6 * geom.nface() * geom.core.refp.fqrule.n * size * ri +
          6 * geom.nface() * geom.core.refp.fqrule.n * geom.core.bounds[lfl] +
          geom.nface() * geom.core.refp.fqrule.n * opposite_local_face[lfl] +
          geom.core.refp.fqrule.n * recv_bfi[lfl] + qi));
        }
        else
        {
          EXPECT_EQ(right[geom.core.refp.fqrule.n * geom.nface() * ri +
                          geom.core.refp.fqrule.n * fi + qi],
                    (real)-1.);
        }
      }

      if (proc_over_face)
        ++recv_bfi[lfl];
    }
  }

  delete[] left;
  delete[] right;
  data.geom.free_dtypes();
}

TEST(geometry_test_channel_nodes, 1, 2, 3, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  u64 qp1  = data.q + 1;
  real elx = data.lx / (real)data.nx;
  real ely = data.ly / (real)data.ny;
  real elz = data.lz / (real)data.nz;
  real ndx = elx / (real)data.q;
  real ndy = ely / (real)data.q;
  real ndz = elz / (real)data.q;
  v3 proc_corner((real)geom.px * (data.lx / (real)data.npx),
                 (real)geom.py * (data.ly / (real)data.npy),
                 (real)geom.pz * (data.lz / (real)data.npz));
  for (u64 iez = 0; iez < geom.core.nz; ++iez)
  {
    for (u64 iey = 0; iey < geom.core.ny; ++iey)
    {
      for (u64 iex = 0; iex < geom.core.nx; ++iex)
      {
        u64 en = geom.core.nx * geom.core.ny * iez + geom.core.nx * iey + iex;
        v3 elem_corner(proc_corner.x + (real)iex * elx,
                       proc_corner.y + (real)iey * ely,
                       proc_corner.z + (real)iez * elz);
        for (u64 inz = 0; inz < data.q + 1; ++inz)
        {
          for (u64 iny = 0; iny < data.q + 1; ++iny)
          {
            for (u64 inx = 0; inx < data.q + 1; ++inx)
            {
              v3 expected_node(elem_corner.x + (ndx * (real)inx),
                               elem_corner.y + (ndy * (real)iny),
                               elem_corner.z + (ndz * (real)inz));
              u64 nn     = qp1 * qp1 * inz + qp1 * iny + inx;
              real* node = geom.core.node(en, nn);
              EXPECT_DOUBLE_EQ(expected_node.x, node[0], 10);
              EXPECT_DOUBLE_EQ(expected_node.y, node[1], 10);
              EXPECT_DOUBLE_EQ(expected_node.z, node[2], 10);
            }
          }
        }
      }
    }
  }
  data.geom.free_dtypes();
}

TEST(geometry_test_mass_matrix_inv, 1, 2, 3, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  ASSERT_EQ(geom.core.nelem, data.nx * data.ny * data.nz / (u64)size);

  u64 pp1   = data.p + 1;
  u64 pp1p3 = pp1 * pp1 * pp1;
  real* M   = new real[pp1p3 * pp1p3];

  for (u64 elem = 0; elem < geom.core.nelem; ++elem)
  {
    // copy inverse into M for inversion back
    u64 iM     = 0;
    real* Minv = geom.core.Minv(elem);
    for (u64 ir = 0; ir < pp1p3; ++ir)
    {
      for (u64 ic = ir; ic < pp1p3; ++ic)
      {
        M[ir * pp1p3 + ic] = Minv[iM];
        M[ic * pp1p3 + ir] = Minv[iM];
        ++iM;
      }
    }

    // invert to normal matrix
    invert(M, pp1p3);

    // find sum of entries
    real sum = 0.;
    for (u64 i = 0; i < pp1p3 * pp1p3; ++i)
      sum += M[i];

    // check (all elements are the same size in this simple case)
    EXPECT_DOUBLE_EQ((data.lx / (real)data.nx) * (data.ly / (real)data.ny) *
                     (data.lz / (real)data.nz),
                     sum, 40);
  }
  delete[] M;
  data.geom.free_dtypes();
}

TEST(geometry_test_jacobian_determinant, 1)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  ASSERT_EQ(geom.core.nelem, data.nx * data.ny * data.nz / (u64)size);

  for (u64 elem = 0; elem < geom.core.nelem; ++elem)
  {
    for (u64 qnn = 0; qnn < geom.core.refp.vqrule.n; ++qnn)
    {
      EXPECT_DOUBLE_EQ((data.lx / (real)data.nx) * (data.ly / (real)data.ny) *
                       (data.lz / (real)data.nz),
                       geom.core.vJ(elem, qnn), 10);
    }
  }
  data.geom.free_dtypes();
}

TEST(geometry_test_interior_gradients, 1, 2, 3, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  ASSERT_EQ(geom.core.nelem, data.nx * data.ny * data.nz / (u64)size);

  for (u64 elem = 0; elem < geom.core.nelem; ++elem)
  {
    for (u64 qnn = 0; qnn < geom.core.refp.vqrule.n; ++qnn)
    {
      real sumx = 0., sumy = 0., sumz = 0.;
      for (u64 bfn = 0; bfn < geom.core.refp.nbf3d; ++bfn)
      {
        real* grad = geom.core.vgrad(elem, bfn, qnn);

        sumx += grad[0];
        sumy += grad[1];
        sumz += grad[2];
      }
      EXPECT_DOUBLE_EQ(sumx, 0., 10, 2e-15);
      EXPECT_DOUBLE_EQ(sumy, 0., 10, 2e-15);
      EXPECT_DOUBLE_EQ(sumz, 0., 10, 2e-15);
    }
  }
  data.geom.free_dtypes();
}

TEST(geometry_test_face_gradients, 1, 2, 3, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  ASSERT_EQ(geom.core.nelem, data.nx * data.ny * data.nz / (u64)size);

  for (u64 elem = 0; elem < geom.core.nelem; ++elem)
  {
    for (u64 lfn = 0; lfn < 6; ++lfn)
    {
      for (u64 qnn = 0; qnn < geom.core.refp.fqrule.n; ++qnn)
      {
        real sumx = 0., sumy = 0., sumz = 0.;
        for (u64 bfn = 0; bfn < geom.core.refp.nbf3d; ++bfn)
        {
          real* grad = geom.core.fgrad(elem, lfn, bfn, qnn);
          sumx += grad[0];
          sumy += grad[1];
          sumz += grad[2];
        }
        EXPECT_DOUBLE_EQ(sumx, 0., 10, 2e-15);
        EXPECT_DOUBLE_EQ(sumy, 0., 10, 2e-15);
        EXPECT_DOUBLE_EQ(sumz, 0., 10, 2e-15);
      }
    }
  }
  data.geom.free_dtypes();
}

v3 expected_normal_box(s64 lfl)
{
  v3 expected_normal;

  switch (lfl)
  {
    case 0: expected_normal = v3(-1., 0., 0.); break;
    case 1: expected_normal = v3(+1., 0., 0.); break;
    case 2: expected_normal = v3(0., -1., 0.); break;
    case 3: expected_normal = v3(0., +1., 0.); break;
    case 4: expected_normal = v3(0., 0., -1.); break;
    case 5: expected_normal = v3(0., 0., +1.); break;
  }

  return expected_normal;
}

real expected_face_jacobian_box(s64 lfl, real lx, real ly, real lz, u64 nx,
                                u64 ny, u64 nz)
{
  real expected_elem = 0.;

  switch (lfl)
  {
    case 0:
    case 1: expected_elem = (ly / (real)ny) * (lz / (real)nz); break;
    case 2:
    case 3: expected_elem = (lx / (real)nx) * (lz / (real)nz); break;
    case 4:
    case 5: expected_elem = (ly / (real)ny) * (lx / (real)nx); break;
  }

  return expected_elem;
}

// Since the geometry for this test is just a box, the normals should be
// generally predictable. The general rule for normal directions on construction
// in geometry is that they always point from lower element index to higher
// element index unless the face is on the -x -y or -z domain boundaries in
// which case this rule would cause an inward pointing normal, the normal is
// flipped in these cases. Since the normal also always points from left to
// right, in most cases the local face on the left must be the high (+x, +y, +z)
// face. The only exception is when the normal is flipped on the low domain
// boundaries, in which case the face on the left must also be the low face of
// the element making the expected normal easy to determine.
TEST(geometry_test_normals, 1, 2, 3, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  for (u64 ifi = 0; ifi < geom.num_interior_faces(); ++ifi)
  {
    s64 fi  = geom.interior_face_list[5 * ifi + 0];
    s64 lfl = geom.interior_face_list[5 * ifi + 3];

    v3 expected_normal = expected_normal_box(lfl);

    for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
    {
      real* n = geom.n(fi, qi);
      EXPECT_DOUBLE_EQ(expected_normal.x, n[0], 10, 1e-15);
      EXPECT_DOUBLE_EQ(expected_normal.y, n[1], 10, 1e-15);
      EXPECT_DOUBLE_EQ(expected_normal.z, n[2], 10, 1e-15);
    }
  }

  for (u64 bfi = 0; bfi < geom.num_boundary_faces(); ++bfi)
  {
    s64 fi  = geom.boundary_face_list[4 * bfi + 0];
    s64 lfl = geom.boundary_face_list[4 * bfi + 3];

    v3 expected_normal = expected_normal_box(lfl);

    for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
    {
      real* n = geom.n(fi, qi);
      EXPECT_DOUBLE_EQ(expected_normal.x, n[0], 10, 1e-15);
      EXPECT_DOUBLE_EQ(expected_normal.y, n[1], 10, 1e-15);
      EXPECT_DOUBLE_EQ(expected_normal.z, n[2], 10, 1e-15);
    }
  }

  data.geom.free_dtypes();
}

TEST(geometry_shared_test_normals, 1)
{
  channel setup;

  shared_geometry geom =
  bump_shared(setup.p, setup.q, setup.nx, setup.ny, setup.nz, setup.lx,
              setup.ly, setup.lz, 0., 0., 0., 0.1, setup.bounds);

  for (u64 ifi = 0; ifi < geom.num_interior_faces(); ++ifi)
  {
    s64 fi  = geom.interior_face_list[5 * ifi + 0];
    s64 lfl = geom.interior_face_list[5 * ifi + 3];

    v3 expected_normal = expected_normal_box(lfl);

    for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
    {
      real* n = geom.n(fi, qi);
      EXPECT_DOUBLE_EQ(expected_normal.x, n[0], 10, 1e-15);
      EXPECT_DOUBLE_EQ(expected_normal.y, n[1], 10, 1e-15);
      EXPECT_DOUBLE_EQ(expected_normal.z, n[2], 10, 1e-15);
    }
  }

  for (u64 bfi = 0; bfi < geom.num_boundary_faces(); ++bfi)
  {
    s64 fi  = geom.boundary_face_list[4 * bfi + 0];
    s64 lfl = geom.boundary_face_list[4 * bfi + 3];

    v3 expected_normal = expected_normal_box(lfl);

    for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
    {
      real* n = geom.n(fi, qi);
      EXPECT_DOUBLE_EQ(expected_normal.x, n[0], 10, 1e-15);
      EXPECT_DOUBLE_EQ(expected_normal.y, n[1], 10, 1e-15);
      EXPECT_DOUBLE_EQ(expected_normal.z, n[2], 10, 1e-15);
    }
  }
}

TEST(geometry_test_surface_elements, 1, 2, 3, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  for (u64 ifi = 0; ifi < geom.num_interior_faces(); ++ifi)
  {
    s64 fi  = geom.interior_face_list[5 * ifi + 0];
    s64 lfl = geom.interior_face_list[5 * ifi + 3];

    real expected_elem = expected_face_jacobian_box(
    lfl, data.lx, data.ly, data.lz, data.nx, data.ny, data.nz);

    for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
      EXPECT_DOUBLE_EQ(geom.fJ(fi, qi), expected_elem, 10);
  }

  for (u64 bfi = 0; bfi < geom.num_boundary_faces(); ++bfi)
  {
    s64 fi  = geom.boundary_face_list[4 * bfi + 0];
    s64 lfl = geom.boundary_face_list[4 * bfi + 3];

    real expected_elem = expected_face_jacobian_box(
    lfl, data.lx, data.ly, data.lz, data.nx, data.ny, data.nz);

    for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
      EXPECT_DOUBLE_EQ(geom.fJ(fi, qi), expected_elem, 10);
  }

  data.geom.free_dtypes();
}

TEST(geometry_shared_test_surface_elements, 1)
{
  channel setup;

  shared_geometry geom =
  bump_shared(setup.p, setup.q, setup.nx, setup.ny, setup.nz, setup.lx,
              setup.ly, setup.lz, 0., 0., 0., 0.1, setup.bounds);

  for (u64 ifi = 0; ifi < geom.num_interior_faces(); ++ifi)
  {
    s64 fi  = geom.interior_face_list[5 * ifi + 0];
    s64 lfl = geom.interior_face_list[5 * ifi + 3];

    real expected_elem = expected_face_jacobian_box(
    lfl, setup.lx, setup.ly, setup.lz, setup.nx, setup.ny, setup.nz);

    for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
      EXPECT_DOUBLE_EQ(geom.fJ(fi, qi), expected_elem, 10);
  }

  for (u64 bfi = 0; bfi < geom.num_boundary_faces(); ++bfi)
  {
    s64 fi  = geom.boundary_face_list[4 * bfi + 0];
    s64 lfl = geom.boundary_face_list[4 * bfi + 3];

    real expected_elem = expected_face_jacobian_box(
    lfl, setup.lx, setup.ly, setup.lz, setup.nx, setup.ny, setup.nz);

    for (u64 qi = 0; qi < geom.core.refp.fqrule.n; ++qi)
      EXPECT_DOUBLE_EQ(geom.fJ(fi, qi), expected_elem, 10);
  }
}

TEST(geometry_shared_periodicity, 1)
{
  channel setup;

  u64 nx = setup.nx;
  u64 ny = setup.ny;
  u64 nz = setup.nz;

  shared_geometry geom =
  bump_shared(setup.p, setup.q, setup.nx, setup.ny, setup.nz, setup.lx,
              setup.ly, setup.lz, 0., 0., 0., 0.1, setup.bounds);

  s64 iL, jL, kL;
  s64 iR, jR, kR;
  s64 iT, jT, kT;
  for (u64 ifi = 0; ifi < geom.num_interior_faces(); ++ifi)
  {
    s64 eL  = geom.interior_face_list[5 * ifi + 1];
    s64 eR  = geom.interior_face_list[5 * ifi + 2];
    s64 lfL = geom.interior_face_list[5 * ifi + 3];

    split3_cartesian<s64>(eL, nx, ny, &iL, &jL, &kL);
    split3_cartesian<s64>(eR, nx, ny, &iR, &jR, &kR);

    switch (lfL)
    {
      case 0:  // -x
        iT = ((iL + nx) - 1) % nx;
        jT = ((jL + ny) + 0) % ny;
        kT = ((kL + nz) + 0) % nz;
        break;
      case 1:  // +x
        iT = ((iL + nx) + 1) % nx;
        jT = ((jL + ny) + 0) % ny;
        kT = ((kL + nz) + 0) % nz;
        break;
      case 2:  // -y
        iT = ((iL + nx) + 0) % nx;
        jT = ((jL + ny) - 1) % ny;
        kT = ((kL + nz) + 0) % nz;
        break;
      case 3:  // +y
        iT = ((iL + nx) + 0) % nx;
        jT = ((jL + ny) + 1) % ny;
        kT = ((kL + nz) + 0) % nz;
        break;
      case 4:  // -z
        iT = ((iL + nx) + 0) % nx;
        jT = ((jL + ny) + 0) % ny;
        kT = ((kL + nz) - 1) % nz;
        break;
      case 5:  // +z
        iT = ((iL + nx) + 0) % nx;
        jT = ((jL + ny) + 0) % ny;
        kT = ((kL + nz) + 1) % nz;
        break;
    }

    EXPECT_EQ(iR, iT);
    EXPECT_EQ(jR, jT);
    EXPECT_EQ(kR, kT);
  }
}

real face_area_uniform_geom(s64 lf, real lx, real ly, real lz, u64 nx, u64 ny,
                            u64 nz)
{
  real face_area = 0.;
  switch (lf)
  {
    case 0:
    case 1: face_area = (ly / (real)ny) * (lz / (real)nz); break;
    case 2:
    case 3: face_area = (lx / (real)nx) * (lz / (real)nz); break;
    case 4:
    case 5: face_area = (lx / (real)nx) * (ly / (real)ny); break;
  }
  return face_area;
}

TEST(geometry_shared_interior_penalty_h, 1)
{
  channel setup;

  shared_geometry geom =
  bump_shared(setup.p, setup.q, setup.nx, setup.ny, setup.nz, setup.lx,
              setup.ly, setup.lz, 0., 0., 0., 0.1, setup.bounds);

  real elem_vol = (setup.lx / (real)setup.nx) * (setup.ly / (real)setup.ny) *
                  (setup.lz / (real)setup.nz);

  // interior test
  for (u64 ifi = 0; ifi < geom.num_interior_faces(); ++ifi)
  {
    s64 fi  = geom.interior_face_list[5 * ifi + 0];
    s64 eL  = geom.interior_face_list[5 * ifi + 1];
    s64 eR  = geom.interior_face_list[5 * ifi + 2];
    s64 lfL = geom.interior_face_list[5 * ifi + 3];
    s64 lfR = geom.interior_face_list[5 * ifi + 4];

    real faL = face_area_uniform_geom(lfL, setup.lx, setup.ly, setup.lz,
                                      setup.nx, setup.ny, setup.nz);
    real faR = face_area_uniform_geom(lfR, setup.lx, setup.ly, setup.lz,
                                      setup.nx, setup.ny, setup.nz);

    real hL = elem_vol / faL;
    real hR = elem_vol / faR;

    EXPECT_DOUBLE_EQ(hL, geom.interior_h[2 * ifi + 0], 10);
    EXPECT_DOUBLE_EQ(hR, geom.interior_h[2 * ifi + 1], 10);
  }

  // boundary test
  for (u64 bfi = 0; bfi < geom.num_boundary_faces(); ++bfi)
  {
    s64 fi  = geom.boundary_face_list[4 * bfi + 0];
    s64 eL  = geom.boundary_face_list[4 * bfi + 1];
    s64 bt  = geom.boundary_face_list[4 * bfi + 2];
    s64 lfL = geom.boundary_face_list[4 * bfi + 3];

    real faL = face_area_uniform_geom(lfL, setup.lx, setup.ly, setup.lz,
                                      setup.nx, setup.ny, setup.nz);

    real hL = elem_vol / faL;

    EXPECT_DOUBLE_EQ(hL, geom.boundary_h[bfi], 10);
  }
}

TEST(geometry_test_Minv_aux_flat_geometry, 1, 2, 3, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);
  distributed_geometry& geom = data.geom;

  real* Minv = geom.core.Minv(0);  // all mass matrices are the same

  for (u64 mi = 0; mi < geom.num_Minv_aux(); ++mi)
  {
    real* Minv_aux = geom.Minv_aux(mi);
    for (u64 i = 0; i < geom.core.num_entry_Minv(); ++i)
      EXPECT_DOUBLE_EQ(Minv[i], Minv_aux[i], 40);
  }

  for (u64 ifi = 0; ifi < geom.num_interior_faces(); ++ifi)
  {
    s64 eR;
    if ((eR = geom.interior_face_list[5 * ifi + 2]) < 0)
    {
      s64 indx_decode = -(eR + 1);
      EXPECT_TRUE(indx_decode > -1 && indx_decode < geom.num_Minv_aux());
    }
  }
  data.geom.free_dtypes();
}

TEST(geometry_test_gather, 1, 2, 3, 4)
{
  int rank, size;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  channel data(comm, size);

  map<simstate> outputs;
  outputs.add("state", simstate(data.geom.core));
  outputs.add("mean", simstate(data.geom.core));
  outputs.add("rms", simstate(data.geom.core));

  simstate& U    = outputs.get("state");
  simstate& mean = outputs.get("mean");
  simstate& rms  = outputs.get("rms");

  // initialize U with code for each entry
  for (u64 ei = 0; ei < data.geom.core.nelem; ++ei)
  {
    u64 gei = data.geom.elem_num_to_global(ei);
    for (u64 bi = 0; bi < data.geom.core.refp.nbf3d; ++bi)
      for (u64 ri = 0; ri < 5; ++ri)
      {
        U(ei, ri, bi) = data.geom.core.refp.nbf3d * 5 * gei + 5 * bi + ri;
        mean(ei, ri, bi) =
        (data.geom.core.refp.nbf3d * 5 * gei + 5 * bi + ri) * 2;
        rms(ei, ri, bi) =
        (data.geom.core.refp.nbf3d * 5 * gei + 5 * bi + ri) * 3;
      }
  }

  core_geometry geom_gathered;
  map<simstate> outputs_gathered;

  gather(data.geom, outputs, geom_gathered, outputs_gathered);

  simstate& U_gathered    = outputs_gathered.get("state");
  simstate& mean_gathered = outputs_gathered.get("mean");
  simstate& rms_gathered  = outputs_gathered.get("rms");

  if (rank == 0)
  {
    distributed_geometry geom_reconstructed = bump_distributed(
    data.p, data.q, data.nx, data.ny, data.nz, data.lx, data.ly, data.lz, 0.,
    0., 0., 0.1, data.bounds, comm, 1, 1, 1, 1);

    simstate U_reconstructed(geom_reconstructed.core);
    simstate mean_reconstructed(geom_reconstructed.core);
    simstate rms_reconstructed(geom_reconstructed.core);
    for (u64 ei = 0; ei < geom_reconstructed.core.nelem; ++ei)
      for (u64 bi = 0; bi < geom_reconstructed.core.refp.nbf3d; ++bi)
        for (u64 ri = 0; ri < 5; ++ri)
        {
          U_reconstructed(ei, ri, bi) =
          geom_reconstructed.core.refp.nbf3d * 5 * ei + 5 * bi + ri;
          mean_reconstructed(ei, ri, bi) =
          (geom_reconstructed.core.refp.nbf3d * 5 * ei + 5 * bi + ri) * 2;
          rms_reconstructed(ei, ri, bi) =
          (geom_reconstructed.core.refp.nbf3d * 5 * ei + 5 * bi + ri) * 3;
        }

    // assert things are the same size
    ASSERT_EQ(geom_reconstructed.core.node_size(), geom_gathered.node_size());
    ASSERT_EQ(geom_reconstructed.nx_glo, geom_gathered.nx);
    ASSERT_EQ(geom_reconstructed.ny_glo, geom_gathered.ny);
    ASSERT_EQ(geom_reconstructed.nz_glo, geom_gathered.nz);
    ASSERT_EQ(geom_reconstructed.core.nx, geom_gathered.nx);
    ASSERT_EQ(geom_reconstructed.core.ny, geom_gathered.ny);
    ASSERT_EQ(geom_reconstructed.core.nz, geom_gathered.nz);
    ASSERT_EQ(geom_reconstructed.core.p, geom_gathered.p);
    ASSERT_EQ(geom_reconstructed.core.q, geom_gathered.q);
    for (u64 i = 0; i < 6; ++i)
    {
      ASSERT_EQ(geom_reconstructed.bounds_glo[i], geom_gathered.bounds[i]);
    }

    ASSERT_EQ(U_reconstructed.size(), U_gathered.size());
    ASSERT_EQ(U_reconstructed.nbfnc, U_gathered.nbfnc);
    ASSERT_EQ(U_reconstructed.nelem, U_gathered.nelem);
    ASSERT_EQ(U_reconstructed.rank, U_gathered.rank);

    for (u64 i = 0; i < geom_reconstructed.core.node_size(); ++i)
    {
      EXPECT_DOUBLE_EQ(geom_reconstructed.core.node_[i], geom_gathered.node_[i],
                       10);
    }

    for (u64 i = 0; i < U_reconstructed.size(); ++i)
    {
      EXPECT_EQ(U_reconstructed[i], U_gathered[i]);
      EXPECT_EQ(mean_reconstructed[i], mean_gathered[i]);
      EXPECT_EQ(rms_reconstructed[i], rms_gathered[i]);
    }

    geom_reconstructed.free_dtypes();
  }
  data.geom.free_dtypes();
}

TEST(geometry_test_scatter, 1, 2, 3, 4)
{
  int size, rank;
  MPI_Comm_size(comm, &size);
  MPI_Comm_rank(comm, &rank);

  // construct single proc version

  channel data_single;
  map<simstate> outputs_single;

  if (rank == 0)
  {
    data_single           = channel(comm, 1);
    simstate& U_single    = outputs_single.get("state");
    simstate& mean_single = outputs_single.get("mean");
    simstate& rms_single  = outputs_single.get("rms");
    U_single              = simstate(data_single.geom.core);
    mean_single           = simstate(data_single.geom.core);
    rms_single            = simstate(data_single.geom.core);

    for (u64 ei = 0; ei < data_single.geom.core.nelem; ++ei)
      for (u64 bi = 0; bi < data_single.geom.core.refp.nbf3d; ++bi)
        for (u64 ri = 0; ri < 5; ++ri)
        {
          U_single(ei, ri, bi) =
          data_single.geom.core.refp.nbf3d * 5 * ei + 5 * bi + ri;
          mean_single(ei, ri, bi) =
          (data_single.geom.core.refp.nbf3d * 5 * ei + 5 * bi + ri) * 2;
          rms_single(ei, ri, bi) =
          (data_single.geom.core.refp.nbf3d * 5 * ei + 5 * bi + ri) * 3;
        }
  }

  // construct multiproc version

  channel data_multiple(comm, size);
  simstate U_multiple(data_multiple.geom.core);
  simstate mean_multiple(data_multiple.geom.core);
  simstate rms_multiple(data_multiple.geom.core);

  for (u64 ei = 0; ei < data_multiple.geom.core.nelem; ++ei)
  {
    u64 gei = data_multiple.geom.elem_num_to_global(ei);
    for (u64 bi = 0; bi < data_multiple.geom.core.refp.nbf3d; ++bi)
      for (u64 ri = 0; ri < 5; ++ri)
      {
        U_multiple(ei, ri, bi) =
        data_multiple.geom.core.refp.nbf3d * 5 * gei + 5 * bi + ri;
        mean_multiple(ei, ri, bi) =
        (data_multiple.geom.core.refp.nbf3d * 5 * gei + 5 * bi + ri) * 2;
        rms_multiple(ei, ri, bi) =
        (data_multiple.geom.core.refp.nbf3d * 5 * gei + 5 * bi + ri) * 3;
      }
  }

  // scatter

  distributed_geometry geom_scattered;
  map<simstate> outputs_scattered;
  scatter(data_single.geom.core, outputs_single, comm, size, data_multiple.npx,
          data_multiple.npy, data_multiple.npz, geom_scattered,
          outputs_scattered);

  // compare...

  simstate& U_scattered    = outputs_scattered.get("state");
  simstate& mean_scattered = outputs_scattered.get("mean");
  simstate& rms_scattered  = outputs_scattered.get("rms");

  for (u64 i = 0; i < data_multiple.geom.core.node_size(); ++i)
  {
    EXPECT_DOUBLE_EQ(data_multiple.geom.core.node_[i],
                     geom_scattered.core.node_[i], 10);
  }

  for (u64 i = 0; i < U_multiple.size(); ++i)
  {
    EXPECT_DOUBLE_EQ(U_multiple[i], U_scattered[i], 10);
    EXPECT_DOUBLE_EQ(mean_multiple[i], mean_scattered[i], 10);
    EXPECT_DOUBLE_EQ(rms_multiple[i], rms_scattered[i], 10);
  }

  data_multiple.geom.free_dtypes();
  if (rank == 0)
    data_single.geom.free_dtypes();
}
